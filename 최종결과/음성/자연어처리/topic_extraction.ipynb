{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4ac74b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "017cea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 Topic: 말투, 만드는, 담긴, 호감형, 누구에게나, 물거품으로, 모여드는, 말투의, 말투다, 만들고\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장\n",
    "# sentence = \"여기는 1번 구역인데, 지금 어어어어 너무 덥고, 습하고, 힘들어요. 쓰러질것 같아요\"\n",
    "sentence = \"기분 좋은 커뮤니케이션을 만드는 사소한 말투의 기적 수백 개의 장점을 한순간에 물거품으로 만드는 것이 있으니,\\\n",
    "            그것이 바로 말투다. 한두 마디에 담긴 말투 하나가 사람의 이미지를 만들고, 관계를 결정하며, 평판에 영향을 미친다.\\\n",
    "            이 책은 그동안 말투 때문에 본의 아니게 관계에서 오해를 불러일으켰던 사람들을 위해 ‘호감형 말투 사용법’을 골라 담았다.\\\n",
    "            이 책에 담긴 말투 사용법을 터득한다면 누구에게나 ‘다시 만나고 싶은 사람’으로 기억될 것이며,\\\n",
    "            주변에 저절로 사람이 모여드는 극적인 변화를 경험하게 될 것이다.\"\n",
    "\n",
    "# 문장을 토큰화\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# 단어들을 다시 문장으로 변환\n",
    "tokens = [\" \".join(tokens)]\n",
    "\n",
    "# 문장을 벡터화\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(tokens)\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA) 모델로 topic 추출\n",
    "lda_model = LatentDirichletAllocation(n_components=1, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# 추출된 topic 출력\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words = lda_model.components_[0].argsort()[:-11:-1]\n",
    "topic_words = [feature_names[i] for i in top_words]\n",
    "print(\"추출된 Topic:\", \", \".join(topic_words))\n",
    "# topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380a3dd",
   "metadata": {},
   "source": [
    "# 다른 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7564c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 1: 0.077*\",\" + 0.077*\"지금\" + 0.077*\"어어어어\" + 0.077*\"1번\" + 0.077*\"구역인데\" + 0.077*\"너무\" + 0.077*\"덥고\" + 0.077*\"여기는\" + 0.077*\"쓰러질것\" + 0.077*\"습하고\"\n",
      "토픽 2: 0.077*\",\" + 0.077*\"지금\" + 0.077*\"어어어어\" + 0.077*\"여기는\" + 0.077*\"구역인데\" + 0.077*\"같아요\" + 0.077*\"1번\" + 0.077*\"덥고\" + 0.077*\"너무\" + 0.077*\"습하고\"\n",
      "토픽 3: 0.077*\",\" + 0.077*\"어어어어\" + 0.077*\"지금\" + 0.077*\"1번\" + 0.077*\"너무\" + 0.077*\"같아요\" + 0.077*\"구역인데\" + 0.077*\"여기는\" + 0.077*\"덥고\" + 0.077*\"힘들어요\"\n",
      "토픽 4: 0.077*\",\" + 0.077*\"지금\" + 0.077*\"어어어어\" + 0.077*\"1번\" + 0.077*\"덥고\" + 0.077*\"구역인데\" + 0.077*\"여기는\" + 0.077*\"쓰러질것\" + 0.077*\"같아요\" + 0.077*\"너무\"\n",
      "토픽 5: 0.190*\",\" + 0.068*\"쓰러질것\" + 0.068*\".\" + 0.068*\"힘들어요\" + 0.068*\"너무\" + 0.068*\"1번\" + 0.067*\"습하고\" + 0.067*\"같아요\" + 0.067*\"여기는\" + 0.067*\"덥고\"\n",
      "토픽 6: 0.077*\",\" + 0.077*\"어어어어\" + 0.077*\"지금\" + 0.077*\"너무\" + 0.077*\"쓰러질것\" + 0.077*\"덥고\" + 0.077*\"구역인데\" + 0.077*\"1번\" + 0.077*\"습하고\" + 0.077*\"여기는\"\n",
      "토픽 7: 0.077*\",\" + 0.077*\"지금\" + 0.077*\"어어어어\" + 0.077*\"구역인데\" + 0.077*\"같아요\" + 0.077*\"너무\" + 0.077*\"1번\" + 0.077*\"여기는\" + 0.077*\"쓰러질것\" + 0.077*\"덥고\"\n",
      "토픽 8: 0.077*\"어어어어\" + 0.077*\",\" + 0.077*\"지금\" + 0.077*\"덥고\" + 0.077*\"여기는\" + 0.077*\"구역인데\" + 0.077*\"1번\" + 0.077*\"같아요\" + 0.077*\"습하고\" + 0.077*\"힘들어요\"\n",
      "토픽 9: 0.077*\",\" + 0.077*\"지금\" + 0.077*\"어어어어\" + 0.077*\"구역인데\" + 0.077*\"같아요\" + 0.077*\"쓰러질것\" + 0.077*\"너무\" + 0.077*\"힘들어요\" + 0.077*\"1번\" + 0.077*\"덥고\"\n",
      "토픽 10: 0.086*\",\" + 0.081*\"어어어어\" + 0.080*\"지금\" + 0.077*\"구역인데\" + 0.077*\"덥고\" + 0.076*\"여기는\" + 0.076*\"같아요\" + 0.076*\"습하고\" + 0.076*\"1번\" + 0.075*\"너무\"\n",
      "문서 1의 주요 토픽: 토픽 5, 확률 0.944, 단어들: 0.190*\",\" + 0.068*\"쓰러질것\" + 0.068*\".\" + 0.068*\"힘들어요\" + 0.068*\"너무\" + 0.068*\"1번\" + 0.067*\"습하고\" + 0.067*\"같아요\" + 0.067*\"여기는\" + 0.067*\"덥고\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 import\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')  # 토큰화를 위해 필요한 NLTK 데이터 다운로드\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 텍스트 데이터 준비\n",
    "text_data = [\"여기는 1번 구역인데, 지금 어어어어 너무 덥고, 습하고, 힘들어요. 쓰러질것 같아요\"]\n",
    "# 텍스트 데이터가 들어있는 리스트\n",
    "\n",
    "# 텍스트 데이터 전처리: 토큰화, 불용어 제거, 등의 전처리 작업 수행\n",
    "tokenized_text_data = [word_tokenize(text) for text in text_data]  # 텍스트 데이터 토큰화\n",
    "\n",
    "# 문서-단어 행렬 생성\n",
    "dictionary = corpora.Dictionary(tokenized_text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text_data]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)  # 10개의 토픽으로 학습\n",
    "\n",
    "# 학습된 토픽 출력\n",
    "for topic_idx, topic_words in lda_model.print_topics():\n",
    "    print(f\"토픽 {topic_idx+1}: {topic_words}\")\n",
    "\n",
    "# 특정 문서의 토픽 추출\n",
    "doc_idx = 0  # 추출하고 싶은 문서의 인덱스\n",
    "doc_bow = corpus[doc_idx]  # 해당 문서의 문서-단어 행렬\n",
    "doc_topics = lda_model.get_document_topics(doc_bow)  # 문서의 토픽 추출\n",
    "doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)  # 토픽의 확률에 따라 정렬\n",
    "\n",
    "# 가장 높은 확률의 토픽 출력\n",
    "top_topic_idx, top_topic_prob = doc_topics[0]\n",
    "top_topic_words = lda_model.print_topic(top_topic_idx)\n",
    "print(f\"문서 {doc_idx+1}의 주요 토픽: 토픽 {top_topic_idx+1}, 확률 {top_topic_prob:.3f}, 단어들: {top_topic_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e930dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise_kernel",
   "language": "python",
   "name": "noise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
